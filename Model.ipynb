{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read data from text files\n",
    "with open('bbc-text.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971235\n",
      "863841\n",
      "2226\n",
      "2225\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "print(len(all_text))\n",
    "print(len(all_text.split()))\n",
    "\n",
    "reviewss_split = all_text.split('\\n')\n",
    "reviews_split=[]\n",
    "for i in reviewss_split:\n",
    "    \n",
    "    reviews_split.append(' '.join([w for w in i.split() if w not in stop_words]))\n",
    "    \n",
    "print(len(reviews_split))\n",
    "del reviews_split[-1]\n",
    "print(len(reviews_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buffy creator joins wonder woman creator buffy vampire slayer take new female superhero signing write direct wonder woman movie joss whedon described dc comics character iconic female heroine time way one met yet said love icons finding behind exploring price power linda carter played character 1970s tv series character wonder woman famed red gold costume born paradise island blessed powers strength flight film produced joel silver producer matrix trilogy one better joss adapt legendary wonder woman comic book character created 1940s dynamic feature film 21st century audiences said silver wonder woman first great female superhero emerge comic books later inspire millions fans television incarnation groundbreaking heroine yet reinvented feature film arena whedon created character buffy screenplay 1992 film distanced movie direction eventually took film bombed give slayer went write executive produce seven seasons cult tv series also produced spinoff angel series currently directing film serenity based shortlived scifi series firefly'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_split[999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rapper kanye west shrewd soul us hiphop star kanye west leads race year grammys 10 nominations rose prominence producing songs artists jayz alicia keys emerged behindthescenes role become artist well producer solo career almost ended began nearfatal car crash left west jaw wired shut 2002 resulting song wire became west first uk hit april 2004 subsequent album college dropout became transatlantic success critically commercially west 26 began rapping teenager chicago school inspired beats rhymes 1980s pioneers run dmc hiphop producer id encouraged west sample old soul rb hits revive updated sound approach would become trademark feel like lot soul old records sample said hear put drums bring new millennium like god one records right blessing leaving chicago art school one year move would later inspire title album west began music career coproducing songs artists mase madd rapper drew attention superstar rapper jayz signed west rocafella record label produce numerous artists roster west work gained mainstream recognition produced singles takeover izzo hova jayz 2001 album blueprint incorporating samples five one doors jackson five want back respectively hits credited injecting soul back hiphop success attracted production work jayz 03 bonnie clyde artists ludacris stand alicia keys know name west announced plans solo album driving home latenight los angeles recording session october 2002 involved car crash left jaw fractured three places anytime hear accident heart sinks thank god still later said steering wheel could two inches would west account accident sampled chaka khan hit fire become heart completed solo album college dropout released last year turns smooth humorous sharp largely avoided clich233d hiphop preoccupation guns girls jewellery wire quickly joined uk us charts slow jamz west collaborated rapper twista ray actor jamie foxx college dropout spawned two uk hits string award nominations west shortlisted 10 grammys including nominations artist album year took best hiphop artist producer album titles last year music black origin mobo awards respected rapper producer influential field hiphop kanye west unlikely regret decision leave college early'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_split[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿tv future in the hands of viewers with home theatre systems  plasma highdefinition tv\n",
      "\n",
      "es us leading trend programmes content delivered viewers via home networks cable satellite telecoms companies broadband service providers front rooms portable devices one talkedabout technologies ces digital personal video recorders dvr pvr settop boxes like us tivo uk sky system allow people record store play pause forward wind tv programmes want essentially technology allows much personalised tv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from string import punctuation\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "punc_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "print(punc_text[:88])\n",
    "\n",
    "print()\n",
    "all_text = ' '.join([w for w in punc_text.split() if w not in stop_words])\n",
    "print(all_text[300:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494175\n"
     ]
    }
   ],
   "source": [
    "# create a list of words\n",
    "words = all_text.split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32956\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "print(len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5474,\n",
       " 2880,\n",
       " 1089,\n",
       " 461,\n",
       " 2915,\n",
       " 854,\n",
       " 7,\n",
       " 548,\n",
       " 160,\n",
       " 2549,\n",
       " 5474,\n",
       " 665,\n",
       " 2880,\n",
       " 1089,\n",
       " 461,\n",
       " 206,\n",
       " 793,\n",
       " 347,\n",
       " 327,\n",
       " 690,\n",
       " 11,\n",
       " 8,\n",
       " 5418,\n",
       " 2296,\n",
       " 127,\n",
       " 1789,\n",
       " 5738,\n",
       " 6509,\n",
       " 3438,\n",
       " 8200,\n",
       " 32,\n",
       " 14,\n",
       " 12,\n",
       " 146,\n",
       " 57,\n",
       " 11,\n",
       " 1162,\n",
       " 32,\n",
       " 14,\n",
       " 347,\n",
       " 793,\n",
       " 589,\n",
       " 8201,\n",
       " 7576,\n",
       " 11,\n",
       " 146,\n",
       " 5475,\n",
       " 206,\n",
       " 47,\n",
       " 2296,\n",
       " 127,\n",
       " 1789,\n",
       " 3042,\n",
       " 2641,\n",
       " 1134,\n",
       " 586,\n",
       " 308,\n",
       " 693,\n",
       " 1117,\n",
       " 2549,\n",
       " 5474,\n",
       " 1679,\n",
       " 2575,\n",
       " 91,\n",
       " 66,\n",
       " 1134,\n",
       " 9,\n",
       " 348,\n",
       " 277,\n",
       " 658,\n",
       " 6502,\n",
       " 402,\n",
       " 548,\n",
       " 793,\n",
       " 14,\n",
       " 431,\n",
       " 548,\n",
       " 2402,\n",
       " 8134,\n",
       " 3622,\n",
       " 184,\n",
       " 1604,\n",
       " 793,\n",
       " 347,\n",
       " 1643,\n",
       " 8,\n",
       " 8,\n",
       " 5475,\n",
       " 206,\n",
       " 91,\n",
       " 4057,\n",
       " 1652,\n",
       " 13096,\n",
       " 1216,\n",
       " 119,\n",
       " 11271,\n",
       " 84,\n",
       " 1351,\n",
       " 4964,\n",
       " 794,\n",
       " 97,\n",
       " 32,\n",
       " 3552,\n",
       " 4566,\n",
       " 592,\n",
       " 2431,\n",
       " 91,\n",
       " 66,\n",
       " 123,\n",
       " 9,\n",
       " 66,\n",
       " 88,\n",
       " 11,\n",
       " 8202,\n",
       " 47,\n",
       " 347,\n",
       " 793,\n",
       " 7002,\n",
       " 276,\n",
       " 206,\n",
       " 111,\n",
       " 690,\n",
       " 11,\n",
       " 8,\n",
       " 278,\n",
       " 3104,\n",
       " 695,\n",
       " 13097]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ints[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  32956\n",
      "\n",
      "Tokenized review: \n",
      " [[20564, 144, 1152, 1134, 45, 1047, 689, 5099, 1186, 4143, 119, 163, 3991, 1340, 1258, 1587, 38, 5, 880, 84, 6433, 293, 91, 18, 16, 114, 2984, 1274, 2401, 535, 382, 1200, 61, 2836, 2985, 1669, 6, 828, 701, 9, 884, 20565, 7, 615, 1492, 977, 373, 1893, 1134, 718, 45, 452, 1410, 2006, 1566, 112, 297, 100, 2630, 841, 5100, 1018, 561, 9, 20566, 828, 2453, 119, 313, 163, 3991, 8797, 5692, 4294, 3250, 26, 7, 5693, 17, 1411, 118, 408, 5, 121, 1356, 67, 4476, 457, 5362, 84, 977, 70, 6434, 73, 2007, 53, 8798, 84, 4, 4477, 1186, 84, 1894, 127, 136, 377, 7, 2909, 36, 124, 1135, 1186, 4295, 5, 457, 5362, 2986, 4, 2573, 15404, 367, 1089, 8799, 1299, 566, 20567, 699, 7, 452, 1410, 2006, 112, 1817, 371, 829, 1976, 1532, 40, 1837, 2454, 4875, 5363, 2402, 215, 7, 2512, 73, 763, 4, 1019, 1055, 124, 739, 585, 8800, 113, 26, 1411, 1733, 397, 75, 989, 98, 18, 16, 17, 3251, 20568, 32, 1379, 12853, 21, 32, 79, 324, 2274, 32, 422, 207, 1976, 1341, 304, 5364, 285, 763, 1105, 17, 2513, 1837, 5363, 256, 681, 1136, 373, 1934, 355, 367, 1934, 1, 1242, 12854, 1837, 1761, 56, 20569, 20570, 1734, 297, 1735, 3252, 1153, 373, 37, 795, 259, 2837, 329, 53, 567, 371, 1, 20571, 20572, 629, 1762, 196, 84, 3851, 84, 85, 38, 5, 317, 373, 70, 880, 15405, 84, 1134, 371, 452, 7, 829, 2402, 10, 36, 15406, 789, 507, 361, 1818, 144, 525, 20573, 116, 5, 317, 70, 880, 1120, 1089, 1452, 333, 58, 1935, 1201, 830, 77, 225, 270, 958, 67, 333, 2455, 681, 1274, 2322, 1588, 6883, 3708, 3080, 8799, 1089, 1934, 157, 351, 1493, 70, 53, 567, 93, 1152, 2, 12854, 907, 89, 3081, 20574, 2233, 8801, 92, 710, 389, 398, 1, 2, 12854, 2514, 382, 1412, 42, 70, 456, 30, 6, 958, 828, 6884, 2453, 28, 11035, 20575, 790, 1186, 84, 1894, 7441, 28, 6, 1434, 6885, 7442, 4144, 1392, 4143, 597, 8797, 3709, 1357, 525, 3992, 3250, 9, 861, 597, 61, 20576, 20577, 6885, 84, 20578, 5693, 8797, 645, 3993, 9, 7, 239, 2006, 84, 112, 20579, 86, 597, 2910, 8797, 61, 20580, 2356, 3709, 5365, 3374, 361, 5101, 39, 4476, 9772, 84, 1936, 674, 257, 101, 245, 2691, 305, 7443, 3852, 719, 3605, 5693, 201, 20581, 371, 5, 67, 1895, 977, 796, 808, 81, 561, 2987, 1300, 1492, 9773, 1567, 5, 880, 70, 70]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2226\n",
      "2225\n",
      "tech\n",
      "2225\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# label conversion\n",
    "labels_split = labels.split('\\n')\n",
    "print(len(labels_split))\n",
    "del labels_split[-1]\n",
    "print(len(labels_split))\n",
    "print((labels_split[0]))\n",
    "\n",
    "\n",
    "encoded_labels = np.array([0 if label == 'sport' else 1 if label == 'business'  else 2 if label == 'politics'   else 3 if label == 'tech'  else 4 for label in labels_split])\n",
    "\n",
    "print(len(encoded_labels))\n",
    "print(encoded_labels[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 2214\n"
     ]
    }
   ],
   "source": [
    "# outlier review stats\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remove any text with zero length from the reviews_ints list and their corresponding label in encoded_labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  2225\n",
      "Number of reviews after removing outliers:  2225\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "[20564   144  1152  1134    45  1047   689  5099  1186  4143   119   163\n",
      "  3991  1340  1258  1587    38     5   880    84  6433   293    91    18\n",
      "    16   114  2984  1274  2401   535   382  1200    61  2836  2985  1669\n",
      "     6   828   701     9   884 20565     7   615  1492   977   373  1893\n",
      "  1134   718    45   452  1410  2006  1566   112   297   100  2630   841\n",
      "  5100  1018   561     9 20566   828  2453   119   313   163  3991  8797\n",
      "  5692  4294  3250    26     7  5693    17  1411   118   408     5   121\n",
      "  1356    67  4476   457  5362    84   977    70  6434    73  2007    53\n",
      "  8798    84     4  4477  1186    84  1894   127   136   377     7  2909\n",
      "    36   124  1135  1186  4295     5   457  5362  2986     4  2573 15404\n",
      "   367  1089  8799  1299   566 20567   699     7   452  1410  2006   112\n",
      "  1817   371   829  1976  1532    40  1837  2454  4875  5363  2402   215\n",
      "     7  2512    73   763     4  1019  1055   124   739   585  8800   113\n",
      "    26  1411  1733   397    75   989    98    18    16    17  3251 20568\n",
      "    32  1379 12853    21    32    79   324  2274    32   422   207  1976\n",
      "  1341   304  5364   285   763  1105    17  2513  1837  5363   256   681\n",
      "  1136   373  1934   355   367  1934     1  1242]\n"
     ]
    }
   ],
   "source": [
    "# Testing the implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "#print(features[:1])\n",
    "#print(len(features))\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "print(len(features))\n",
    "print((features[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1557, 200) \n",
      "Validation set: \t(334, 200) \n",
      "Test set: \t\t(334, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.7\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of  resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaders and Batching\n",
    "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
    "\n",
    "Create a known format for accessing our data, using TensorDataset which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 5\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([5, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,   836,   301,  1169,  7945,   255,   503,    42,  7945,\n",
      "           184,   239,  3899,   146,    83,  3525,   179,   301,  3762,  1212,\n",
      "           836,   564,   154,  2031,     1,    87,   571,  5383,   426,  1901,\n",
      "          8710, 18296, 13557,  4756,   116,   267,  1069,   409,   438,  7945,\n",
      "           503,   652,  1170,   415, 25230,  2926,  3885,  3619,     6,     8,\n",
      "         25231,  1991,  6464,  2855,    57,   836,   160,  1539,   217,  2179,\n",
      "          1295,   112,  1025,  2476,  1195,   215,   890,   310,   431, 25232,\n",
      "           409,   178,   501,    30,     5,   948,  1904,  2031,    83,  5858,\n",
      "          5512,  5381,   137,  3240,   419,    15,     1,  4059,  2933,  3762,\n",
      "           383, 25233,  6120,  5419,    53, 25234,  7394,   836,   276,  1901,\n",
      "          8710, 13894,  8169,  1486,   887,  1387,   301,    10, 25235,  4174,\n",
      "           571,  1265,   409,  5784,   104,    69, 15644,  9912,    21,  1989,\n",
      "          1373,   149,    85,  8163,   203,    87,  3312, 25236,  8710,  1884,\n",
      "           241,   111,   712,   490,  5558,  5383,   392,  2205,     2,  9912,\n",
      "             1,   460,   853,    69,  9615,  9616,     1,   306,  1373,   149,\n",
      "            92,   486,  8161,  5383, 11907,   203,   281,   323,   327,   206],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,  4860,   228,\n",
      "          2906,  4134,    25,   460,   572,  7948,  4860,   228,   160,    25,\n",
      "          7577,    67,  1814,   332,    25,  3620,  2906,  4134,  1086,   491,\n",
      "           556,   477,  3380,    95,  3949,  1892,  1838,  6700,   150,     4,\n",
      "           637,  1897,  3857,  4860,    67,  3857, 14211,  1747,   114,  2669,\n",
      "          2599,   278,  1275,   219,  5402,  1838,  9298,  7832,  7832,     4,\n",
      "          1358,  3950,  4860,  7577,   468,   597,   572,  2493,     4,  3275,\n",
      "           332,   254, 17477,    25,  4612,  1180,   358,   586,  4882,   109,\n",
      "          6700,   150,  3464,  4860,    68, 10372,   460,   572,   127,   527,\n",
      "           849,   228,    17,   572,  2822, 17478,   816,  1664,  2269,   254,\n",
      "          1294, 14212, 14213, 12135, 14214,  3689,  2215,   556,   450,   354,\n",
      "           920,    66,  1226,  5628,   430,  3838,  2920,  6701,  9482, 10627,\n",
      "         10628,  3747,  1890, 14215,  1863,  1380,  1892,   476,   679,  4591,\n",
      "          5737,    36,   128, 12136,   767,  1996,   491,  2186,   556,  1109,\n",
      "         17479,  1241,   241,   569,    43,  1345,   712,   168,   833,   156],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,  1732,  1658,   560,   428,  5562,   576,\n",
      "           591,  4253,  1732,  5583,  1659,  1402,   579,   668,   560,   149,\n",
      "          1944,   109,  5284, 13502,  2057,   484,     6, 11605,  1228,  2508,\n",
      "          1732,   243,   319,   108,  2613,   143, 13502,    21,    32,   517,\n",
      "          2015,   784,   560,   414,    18,  4253, 11606,   554,  2823,  1668,\n",
      "          2364,   282, 13502,    37, 11607,  1837,     6,  1228,   151,   262,\n",
      "          9858,   137,  2202,     6,  9858,   151,  3056,  2252,   137,  1739,\n",
      "         13503,  1228,     1,   256,  4208,  1402,   151,   128,   913,  1228],\n",
      "        [    0,     0,     0,     0,     0,     0,   453,  7112,    55,  1966,\n",
      "           251,   453,   349,   543,    72, 21529,   535,   198,   455,  3670,\n",
      "           993,   251,   225,  1854,   590,  5827,  2107,    46,  1225, 16084,\n",
      "           251,   455,  3670,   734,  1250,   453, 21530, 16085,  2506,  5007,\n",
      "          3039,  2946,  1200,   568,   441,   282,   475,  3064,  1307,  1713,\n",
      "         13323,  1881,    36, 10136,   596,  8302,    79,  1400,  1455,   455,\n",
      "          3670,     1,   449,   453,   590,   665,    36,  1966,   251,     1,\n",
      "           453, 21531,  7672, 21532,   191,   218, 21533,   878, 10137,  2923,\n",
      "          1563,  1225,   473,   304,   453,   590,   665,   685, 16085, 16086,\n",
      "          2041,   195, 21534,   191,  4390,     2,  8302,     1,   453,  5886,\n",
      "           195,   878,  2923, 16086,   179,   816,  6228,   144,    45,   808,\n",
      "            44,     4,   210,     9,    12,    63,  8303, 21535,   590,   320,\n",
      "         21536,     6,    73,   453,   253,  4083, 16087,   129,  2669,  3131,\n",
      "             1,     2,  8302,   198,   770,   455,  3670,   734,  7113,   685,\n",
      "             9, 21537, 21538, 21539,   119,  1046,   436,  2338,  1000,   257,\n",
      "           191,   218,    94,   148,   251,   436,   141,     8,  4796,  2640,\n",
      "           455,  3670,   198,  7114,  2592,    14,  6095,    12,  1629,   993,\n",
      "          2385,  8267,    74,  1163,  2600,     2,  8302,     1,   817,    30,\n",
      "             5,  1163, 11475,   100,   198,  1486,    28,   135,  5734,  1859,\n",
      "           163,   773,  3140,  1429,  1911,   251,   164, 11476,     8, 21540],\n",
      "        [   81,   997,   319,  2496,    28,   214,   436,  1046,  1009,  3795,\n",
      "          6115,   520,     3,    19,  3259,  1684,    81,   145,  1046,  1357,\n",
      "           859,   117, 15522,  5888,   181,  1733,    92, 19730, 28773,  1350,\n",
      "          2599,  1261, 12755,  2685,    79,  2228,  3259,  2114,  6328,  8626,\n",
      "            16,  7138,  3744,   333,  2555,   289,    43,   253,   381,     6,\n",
      "          8361,    81,   145,  9585,   213,   813,   631,   499,    92,     9,\n",
      "          6963,   767,  2014,  7735,  6328,  1046,   339,   116,  3795,  3744,\n",
      "            67,    10,   155,  1492,  1647,   500,  3038,   145,  1293,   210,\n",
      "          3021,   163,   339,  3795,   881,     5,   155,  3480,   958,   129,\n",
      "            11,    65,    13,   191,  2430,  3806,  2183,    25,  1369, 10573,\n",
      "          1407,  5341,  8188,   213,  1064,    25,  3372,  1403,     9,   130,\n",
      "           221,   985, 19731,  1097,  5543,   767,   743,    79,  1121,   997,\n",
      "           349,    81,   145,     2,  1407,  5341,   799,  2652,  1904,   997,\n",
      "           843,     1,   985, 19731,  1666,  1455,  1013, 28774,  1003,   997,\n",
      "           145,   120, 19503,    79,  1206,   335,  2046,  2712, 28775,  1922,\n",
      "           746,   129,  1056,   148,   997,  3039,  3510,  1366,  3973,   148,\n",
      "          5212,   491,  1910,  1097,  5302, 19377,    14, 14886,  9873,   142,\n",
      "          5295,  5000,  1615,   959,  1346,  3325,   980,  1848,   491,  6683,\n",
      "          2569,  1457, 28776, 28777,  1562,   191,  2532, 12755,  2228,   373,\n",
      "          2517,     5,     4, 15172,  9565,   462,   164,  3738,  7762,   201]],\n",
      "       dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([5])\n",
      "Sample label: \n",
      " tensor([1, 4, 0, 3, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Text Classification.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sof = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # logSoftmax\n",
    "        sof_out = self.sof(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sof_out = sof_out.view(batch_size, -1)\n",
    "        sof_out = sof_out[:,-5:] # get last 5 class batch of labels\n",
    "        \n",
    "        # return last softmax output and hidden state\n",
    "        return sof_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(32957, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (sof): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 5\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 1/30..  Training Loss: 1.480..  Test Loss: 1.695..  Test Accuracy: 0.260\n",
      "Validation loss decreased (inf --> 113.540260).  Saving model ...\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 2/30..  Training Loss: 1.107..  Test Loss: 1.133..  Test Accuracy: 0.624\n",
      "Validation loss decreased (113.540260 --> 75.890762).  Saving model ...\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 3/30..  Training Loss: 0.538..  Test Loss: 0.800..  Test Accuracy: 0.737\n",
      "Validation loss decreased (75.890762 --> 53.606377).  Saving model ...\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 4/30..  Training Loss: 0.214..  Test Loss: 0.910..  Test Accuracy: 0.746\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 5/30..  Training Loss: 0.098..  Test Loss: 0.946..  Test Accuracy: 0.713\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 6/30..  Training Loss: 0.032..  Test Loss: 0.955..  Test Accuracy: 0.752\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 7/30..  Training Loss: 0.007..  Test Loss: 0.987..  Test Accuracy: 0.743\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 8/30..  Training Loss: 0.003..  Test Loss: 1.021..  Test Accuracy: 0.767\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 9/30..  Training Loss: 0.001..  Test Loss: 1.038..  Test Accuracy: 0.764\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 10/30..  Training Loss: 0.000..  Test Loss: 1.041..  Test Accuracy: 0.773\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 11/30..  Training Loss: 0.000..  Test Loss: 1.045..  Test Accuracy: 0.776\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 12/30..  Training Loss: 0.000..  Test Loss: 1.041..  Test Accuracy: 0.797\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 13/30..  Training Loss: 0.000..  Test Loss: 1.074..  Test Accuracy: 0.797\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 14/30..  Training Loss: 0.000..  Test Loss: 1.062..  Test Accuracy: 0.797\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 15/30..  Training Loss: 0.000..  Test Loss: 1.121..  Test Accuracy: 0.794\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 16/30..  Training Loss: 0.000..  Test Loss: 1.113..  Test Accuracy: 0.803\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 17/30..  Training Loss: 0.000..  Test Loss: 1.144..  Test Accuracy: 0.806\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 18/30..  Training Loss: 0.000..  Test Loss: 1.156..  Test Accuracy: 0.803\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 19/30..  Training Loss: 0.000..  Test Loss: 1.202..  Test Accuracy: 0.803\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 20/30..  Training Loss: 0.000..  Test Loss: 1.183..  Test Accuracy: 0.815\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 21/30..  Training Loss: 0.000..  Test Loss: 1.126..  Test Accuracy: 0.821\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 22/30..  Training Loss: 0.000..  Test Loss: 1.193..  Test Accuracy: 0.815\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 23/30..  Training Loss: 0.000..  Test Loss: 1.219..  Test Accuracy: 0.818\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 24/30..  Training Loss: 0.000..  Test Loss: 1.213..  Test Accuracy: 0.824\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 25/30..  Training Loss: 0.000..  Test Loss: 1.240..  Test Accuracy: 0.815\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 26/30..  Training Loss: 0.000..  Test Loss: 1.256..  Test Accuracy: 0.815\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 27/30..  Training Loss: 0.000..  Test Loss: 1.274..  Test Accuracy: 0.821\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 28/30..  Training Loss: 0.000..  Test Loss: 1.302..  Test Accuracy: 0.830\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 29/30..  Training Loss: 0.000..  Test Loss: 1.314..  Test Accuracy: 0.824\n",
      "Training - Input Shape Issue: torch.Size([2, 200])\n",
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Epoch: 30/30..  Training Loss: 0.000..  Test Loss: 1.353..  Test Accuracy: 0.830\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs =30 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "# initialize tracker for minimum validation loss\n",
    "val_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "\n",
    "val_losses, train_losses = [] , []\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    running_loss=0\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        if( (inputs.shape[0],inputs.shape[1]) != (batch_size,seq_length)):\n",
    "            print('Training - Input Shape Issue:',inputs.shape)\n",
    "            continue\n",
    "      \n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.long())\n",
    "        running_loss += criterion(output.squeeze(), labels.long())\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        val_loss=0\n",
    "        accuracy = 0\n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            \n",
    "            for inputs, labels in valid_loader:\n",
    "                if( (inputs.shape[0],inputs.shape[1]) != (batch_size,seq_length)):\n",
    "                    print('Validation - Input Shape Issue:',inputs.shape)\n",
    "                    continue\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                if(train_on_gpu):\n",
    "                \n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                \n",
    "                \n",
    "                val_loss += criterion(output.squeeze(), labels.long())\n",
    "            \n",
    "                \n",
    "                ps = torch.exp(output)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape).long()\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        val_losses.append(val_loss/len(valid_loader))\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "\n",
    "\n",
    "        net.train()\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
    "                  \"Test Loss: {:.3f}.. \".format(val_loss/len(valid_loader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "        \n",
    "        # save model if validation loss has decreased\n",
    "        if val_loss <= val_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            val_loss_min,\n",
    "            val_loss))\n",
    "            torch.save(net.state_dict(), 'model.pt')\n",
    "            val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.4803, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1068, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.5383, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.2139, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0982, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0315, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0070, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0028, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.8674e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(8.2131e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(7.0070e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(5.7513e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.5879e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7642e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3697e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6413e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.3052e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.8828e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.8419e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3426e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1564e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0583e-05, device='cuda:0', grad_fn=<DivBackward0>), tensor(9.2635e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.6290e-06, device='cuda:0', grad_fn=<DivBackward0>), tensor(5.8339e-06, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "[tensor(1.6946, device='cuda:0'), tensor(1.1327, device='cuda:0'), tensor(0.8001, device='cuda:0'), tensor(0.9104, device='cuda:0'), tensor(0.9463, device='cuda:0'), tensor(0.9550, device='cuda:0'), tensor(0.9871, device='cuda:0'), tensor(1.0209, device='cuda:0'), tensor(1.0382, device='cuda:0'), tensor(1.0414, device='cuda:0'), tensor(1.0445, device='cuda:0'), tensor(1.0408, device='cuda:0'), tensor(1.0742, device='cuda:0'), tensor(1.0625, device='cuda:0'), tensor(1.1211, device='cuda:0'), tensor(1.1134, device='cuda:0'), tensor(1.1442, device='cuda:0'), tensor(1.1564, device='cuda:0'), tensor(1.2018, device='cuda:0'), tensor(1.1830, device='cuda:0'), tensor(1.1261, device='cuda:0'), tensor(1.1933, device='cuda:0'), tensor(1.2190, device='cuda:0'), tensor(1.2134, device='cuda:0'), tensor(1.2397, device='cuda:0'), tensor(1.2562, device='cuda:0'), tensor(1.2741, device='cuda:0'), tensor(1.3017, device='cuda:0'), tensor(1.3145, device='cuda:0'), tensor(1.3526, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses)\n",
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19fe55ce4e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 380
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Input Shape Issue: torch.Size([4, 200])\n",
      "Test Loss: 1.353..  Test Accuracy: 0.833\n"
     ]
    }
   ],
   "source": [
    "test_loss=0\n",
    "accuracy = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "        \n",
    "        \n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        \n",
    "        if( (inputs.shape[0],inputs.shape[1]) != (batch_size,seq_length)):\n",
    "            print('Validation - Input Shape Issue:',inputs.shape)\n",
    "            continue\n",
    "        test_h = tuple([each.data for each in val_h])\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            output, test_h = net(inputs, test_h)\n",
    "                \n",
    "                \n",
    "        test_loss += criterion(output.squeeze(), labels.long())\n",
    "            \n",
    "                \n",
    "        ps = torch.exp(output)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape).long()\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "\n",
    "print(\"Test Loss: {:.3f}.. \".format(val_loss/len(test_loader)),\n",
    "                \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sport test review\n",
    "test_review = 'halo 2  to to to heralds traffic explosion growing popularity online gaming could spell problems net service firms warns network monitoring company sandvine issued warning following analysis shows traffic xbox game network increased fourfold launch day halo 2 9 november traffic explosion continued december said sandvine service providers need make sure networks cope increasing demands bandwidth well popular singleplayer title halo 2 connected microsoft subscriptionbased broadband network xbox live gamers want play online create clan team take others see well compare surge numbers huge demands bandwidth wakeup call industry must ensure networks cope increases traffic said sandvine chief technology officer marc morin bid cope ease congestion providers increasingly making networks intelligent finding using bandwidth could become common charge people amount bandwidth use explosion xbox live traffic attributed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2111, 292, 11086, 1230, 4690, 585, 2195, 164, 539, 10, 3871, 341, 107, 100, 131, 5719, 367, 2922, 44, 12909, 1418, 1174, 268, 2766, 537, 1230, 1210, 23, 367, 842, 15489, 580, 130, 2111, 292, 1572, 411, 1230, 4690, 1058, 217, 1, 12909, 100, 2630, 117, 19, 559, 452, 2196, 1300, 1984, 3732, 40, 518, 20649, 380, 2111, 292, 3177, 257, 20650, 297, 367, 1210, 479, 1142, 70, 67, 164, 622, 11087, 151, 36, 504, 75, 40, 6451, 3273, 698, 395, 1984, 3732, 12910, 553, 115, 316, 620, 452, 2196, 1647, 1230, 1, 12909, 101, 73, 1510, 4681, 20651, 391, 2196, 2767, 11088, 2630, 1329, 213, 452, 4898, 1157, 180, 3732, 10, 210, 1245, 728, 5, 774, 3732, 63, 4690, 1210, 479, 1230, 5116]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "    rm_stop_words = [w for w in test_text.split() if w.lower() not in stop_words]\n",
    "    \n",
    "    test_words=' '.join(rm_stop_words)\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_words.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0  2111   292 11086  1230  4690   585  2195   164\n",
      "    539    10  3871   341   107   100   131  5719   367  2922    44 12909\n",
      "   1418  1174   268  2766   537  1230  1210    23   367   842 15489   580\n",
      "    130  2111   292  1572   411  1230  4690  1058   217     1 12909   100\n",
      "   2630   117    19   559   452  2196  1300  1984  3732    40   518 20649\n",
      "    380  2111   292  3177   257 20650   297   367  1210   479  1142    70\n",
      "     67   164   622 11087   151    36   504    75    40  6451  3273   698\n",
      "    395  1984  3732 12910   553   115   316   620   452  2196  1647  1230\n",
      "      1 12909   101    73  1510  4681 20651   391  2196  2767 11088  2630\n",
      "   1329   213   452  4898  1157   180  3732    10   210  1245   728     5\n",
      "    774  3732    63  4690  1210   479  1230  5116]]\n"
     ]
    }
   ],
   "source": [
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    ps = torch.exp(output)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(ps.squeeze()) \n",
    "    top_p, pred = ps.topk(1, dim=1)\n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(top_p.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if (pred.item()==0):\n",
    "        print(\"sports\")\n",
    "    elif(pred.item()==1) :\n",
    "        print(\"business\")\n",
    "    elif(pred.item()==2) :\n",
    "        print(\"politics\")\n",
    "    \n",
    "    elif(pred.item()==3) :\n",
    "        print(\"tech\")\n",
    "        \n",
    "    else :print(\"Entertainment\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive test review\n",
    "business_text = 'crude oil prices back above $50 cold weather across parts of the united states and much of europe has pushed us crude oil prices above $50 a barrel for the first time in almost three months.  freezing temperatures and heavy snowfall have increased demand for heating fuel in the us  where stocks are low. fresh falls in the value of the dollar helped carry prices above the $50 mark for the first time since november. a barrel of us crude oil closed up $2.80 to $51.15 in new york on tuesday. opec members said on tuesday that it saw no reason to cut its output.  although below last year s peak of $55.67 a barrel  which was reached in october  prices are now well above 2004 s average of $41.48.  brent crude also rose in london trading  adding $1.89 to $48.62 at the close. much of western europe and the north east of america has been shivering under unseasonably low temperatures in recent days. the decline in the us dollar to a five-week low against the euro has also served to inflate prices.  the dollar moved sharply overnight and oil is following it   said chris furness  senior market strategist at 4cast.  if the dollar continues to weaken  oil will be obviously higher.   several opec members said a cut in production was unlikely  citing rising prices and strong demand for oil from asia.  i agree that we do not need to cut supply if the prices are as much as this   fathi bin shatwan  libya s oil minister  told reuters.  i do not think we need to cut unless the prices are falling below $35 a barrel   he added. opec closely watches global stocks to ensure that there is not an excessive supply in the market. the arrival of spring in the northern hemisphere will focus attention on stockpiles of us crude and gasoline  which are up to 9% higher than at this time last year. heavy stockpiles could help force prices lower when demand eases'\n",
    "tech_text='tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets'\n",
    "sports_text='tigers wary of farrell  gamble  leicester say they will not be rushed into making a bid for andy farrell should the great britain rugby league captain decide to switch codes.   we and anybody else involved in the process are still some way away from going to the next stage   tigers boss john wells told bbc radio leicester.  at the moment  there are still a lot of unknowns about andy farrell  not least his medical situation.  whoever does take him on is going to take a big  big gamble.  farrell  who has had persistent knee problems  had an operation on his knee five weeks ago and is expected to be out for another three months. leicester and saracens are believed to head the list of rugby union clubs interested in signing farrell if he decides to move to the 15-man game.  if he does move across to union  wells believes he would better off playing in the backs  at least initially.  i m sure he could make the step between league and union by being involved in the centre   said wells.  i think england would prefer him to progress to a position in the back row where they can make use of some of his rugby league skills within the forwards.  the jury is out on whether he can cross that divide.  at this club  the balance will have to be struck between the cost of that gamble and the option of bringing in a ready-made replacement'\n",
    "entertainment_text='rem announce new glasgow concert us band rem have announced plans to perform for 10 000 scottish fans in a rescheduled gig.  the band will play in what has been dubbed europe s biggest tent on glasgow green on tuesday  14 june. they were forced to pull out of a concert at the secc in glasgow last month after bassist mike mills contracted flu. fans who bought tickets for the original 22 february show can attend the rescheduled concert. the june gig will act as a warm-up for rem s open air concert at balloch castle country park  on the banks of loch lomond  four days later.  promoters regular music booked glasgow green as the secc was not available on the most suitable date. mark mackie  director of regular music  said:  it is fantastic news and it really shows rem s commitment to their scottish fans that they are coming back to glasgow for what will be a truly unique gig.  the rem gigs will kick-start what promises to be a memorable summer for scottish music lovers.  grammy award winners u2 will play hampden on 21 june while oasis will also perform at the national stadium in glasgow on 29 june. coldplay have announced a concert at bellahouston park in glasgow on 1 july and t in the park will be held at balado  near kinross  from 9-10 july. ticketweb and the secc box office will write to customers who bought tickets for the february gig asking if they want to attend the new show. those who bought tickets'\n",
    "politics_text='teens  know little  of politics teenagers questioned for a survey have shown little interest in politics - and have little knowledge.  only a quarter of 14-16 year olds knew that labour was the government  the tories were the official opposition and the lib dems were the third party. almost all could identify tony blair  but only one in six knew who michael howard was  and just one in 10 recognised charles kennedy. the icm survey interviewed 110 pupils for education watchdog ofsted. nearly half those pupils polled said it was not important for them to know more about what the political parties stand for. and 4% of those questioned thought the conservatives were in power - while 2% of them believed the lib dems were. the survey also looked at issues of nationality. it found the union flag and fish and chips topped the list of symbols and foods associated with being british. many of the pupils also looked on themselves as english  scottish or welsh  rather than british; while the notion of being european hardly occurred to anyone'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.999999\n",
      "business\n",
      "None \n",
      "\n",
      "Prediction value, pre-rounding: 0.999986\n",
      "tech\n",
      "None \n",
      "\n",
      "Prediction value, pre-rounding: 0.999998\n",
      "sports\n",
      "None \n",
      "\n",
      "Prediction value, pre-rounding: 0.999998\n",
      "Entertainment\n",
      "None \n",
      "\n",
      "Prediction value, pre-rounding: 0.999997\n",
      "politics\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "print(predict(net,business_text, seq_length),'\\n')\n",
    "\n",
    "print(predict(net,tech_text, seq_length),'\\n')\n",
    "print(predict(net,sports_text, seq_length),'\\n')\n",
    "print(predict(net,entertainment_text, seq_length),'\\n')\n",
    "print(predict(net,politics_text, seq_length),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THANK YOU !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
